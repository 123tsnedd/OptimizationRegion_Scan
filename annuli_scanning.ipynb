{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "import math\n",
    "from skimage.transform import rotate\n",
    "from scipy.ndimage import shift, center_of_mass\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "from typing import Optional\n",
    "from scipy import ndimage\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from CNN_main import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_image(ax, title, image_full, image_cropped, SR_m, masking, xsp_i=None, ysp_i=None, annotations:str=None, vmin=None, vmax=None):\n",
    "    # Plot the full image with colorbar\n",
    "    im = ax[0].imshow(image_full, cmap='viridis', norm=Normalize(vmin=vmin, vmax=vmax))\n",
    "    ax[0].set_title(title)\n",
    "    ax[0].contour(SR_m, levels=[0.5], colors='red')\n",
    "    ax[0].contour(masking)\n",
    "    ax[0].invert_yaxis()\n",
    "    plt.colorbar(im, ax=ax[0])  # Add colorbar here\n",
    "\n",
    "    # Plot the cropped image\n",
    "    ax[1].imshow(image_cropped, vmin= vmin, vmax=vmax, cmap='viridis')\n",
    "    ax[1].set_title(f\"{title} annuli\")\n",
    "    ax[1].invert_yaxis()\n",
    "\n",
    "\n",
    "    if annotations:\n",
    "        ax[0].annotate(annotations, xy=(xsp_i, ysp_i), xytext=(3, 10), arrowprops=dict(facecolor='black', shrink=0.5))\n",
    "\n",
    "def calculate_new_pixel_location(XSP_i, YSP_i, center_x, center_y, angle):\n",
    "    \"\"\"\n",
    "    Calculates the new pixel location after rotation.\n",
    "    \"\"\"\n",
    "    # Convert the angle to radians for rotation matrix\n",
    "    theta = -np.radians(angle)\n",
    "    \n",
    "    # Apply the 2D rotation matrix to find the new coordinates\n",
    "    new_x = center_x + np.cos(theta) * (XSP_i - center_x) - np.sin(theta) * (YSP_i - center_y)\n",
    "    new_y = center_y + np.sin(theta) * (XSP_i - center_x) + np.cos(theta) * (YSP_i - center_y)\n",
    "\n",
    "    return new_x, new_y\n",
    "\n",
    "\n",
    "def search_region(image, srpix_inner, srpix_outer, wedge_angle):\n",
    "    center = ((image.shape[0] - 1) / 2, (image.shape[1] - 1) / 2)  # floats\n",
    "    y, x = np.ogrid[:image.shape[0], :image.shape[1]]\n",
    "    cy, cx = center  #full image center\n",
    "\n",
    "    # Calculate distances from the center\n",
    "    distance = np.sqrt((x - cx) ** 2 + (y - cy) ** 2)\n",
    "\n",
    "    # Create search region\n",
    "    SR_mask = (distance > srpix_inner) & (distance < srpix_outer)\n",
    "\n",
    "    # Calculate angles\n",
    "    angle = np.arctan2(y - cy, x - cx)\n",
    "    angle = (angle + 2 * np.pi) % (2 * np.pi)\n",
    "\n",
    "    # Define north or upward direction angle (270 degrees in radians) on full image\n",
    "    north_angle = np.pi / 2\n",
    "\n",
    "    # Create the OR mask\n",
    "    OR_mask = (angle > north_angle - wedge_angle / 2) & (angle < north_angle + wedge_angle / 2) & SR_mask\n",
    "    \n",
    "    #calc geometric center of or\n",
    "    OR_center_y = cy + 0.5 * (srpix_inner + srpix_outer) * np.sin(north_angle)\n",
    "    OR_center_x = cx + 0.5 * (srpix_inner + srpix_outer) * np.cos(north_angle)\n",
    "    OR_center = (OR_center_y, OR_center_x)\n",
    "    #print(f\"or size: {OR_mask.shape}\")\n",
    "    #print(f\"or_center: {OR_center}\")\n",
    "\n",
    "    #print(cy, cx)\n",
    "    return SR_mask, OR_mask, OR_center\n",
    "\n",
    "def process_search_region(image, srpix_inner, srpix_outer,\n",
    "                          wedge_angle, vmin, vmax, step:int=None,\n",
    "                          annuli_save:bool=False, plot_images:bool=True):\n",
    "    XAP = 0\n",
    "    YAP = 0.5 * (srpix_outer + srpix_inner)\n",
    "\n",
    "    center_y, center_x = ((image.shape[0] - 1) / 2, (image.shape[1] - 1) / 2)\n",
    "    SR_m, OR_m, OR_center = search_region(image, srpix_inner, srpix_outer, wedge_angle)\n",
    "    #print(f\"ormask: {OR_m.shape}\")\n",
    "    processed_images = []\n",
    "    processed_knpixvals = []\n",
    "    count = 0\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            if OR_m[y, x]:\n",
    "                XSP_i, YSP_i = x, y\n",
    "                ISPi = image[YSP_i, XSP_i]\n",
    "                # print(f\"starting knpixval: {ISPi}\" )\n",
    "                # Create a copy of the original image to blackout pixels\n",
    "                image_copy = image.copy()#only for testing purposes\n",
    "                \n",
    "                # Black out the first pixel and surrounding pixels\n",
    "                radius = 2  # Define the radius of surrounding pixels to black out\n",
    "                for dy in range(-radius, radius +1):\n",
    "                    for dx in range(-radius, radius +1):\n",
    "                        ny, nx = YSP_i + dy, XSP_i + dx\n",
    "                        if 0 <= ny < image.shape[0] and 0 <= nx < image.shape[1]:\n",
    "                            image_copy[ny, nx] = 0\n",
    "\n",
    "                angle = np.degrees(np.arctan2( YSP_i - center_y, XSP_i - center_x )) - 90\n",
    "                #print(f\"angle: {angle}\")\n",
    "\n",
    "                rotated_image = rotate(image_copy, angle= angle, order=3, resize=False, preserve_range=True, center=(center_x, center_y))\n",
    "            # calc new pixel loc\n",
    "                new_x, new_y = calculate_new_pixel_location(XSP_i, YSP_i, center_x, center_y, angle)\n",
    "                #print(f\"New pixel location after rotation: ({new_x}, {new_y})\")\n",
    "\n",
    "                # Calculate shift to move new pixel location to the Analysis Point (XAP, YAP)\n",
    "                shift_x = 0\n",
    "                shift_y =  -1 * ((new_y - center_y) - (OR_center[0] - center_y))   #rotated_or_center_y - new_y  #or_center is taken from original image\n",
    "                shifted_image = shift(rotated_image, shift=[shift_y, shift_x], order=3)\n",
    "                #print(f\"Shifting by (y,x): ({shift_y}, {shift_x})\")\n",
    "\n",
    "                #print(f\"center_y: {center_y}., new_y: {new_y}., OR_center: {OR_center[0]} \")\n",
    "                #just the cutouts\n",
    "                masked_image = image_copy * OR_m\n",
    "                masked_image_rotated = rotated_image * OR_m\n",
    "                masked_image_shifted = shifted_image * OR_m\n",
    "\n",
    "                non_zero_rows = np.any(masked_image, axis=1)\n",
    "                non_zero_cols = np.any(masked_image, axis=0)\n",
    "                rot_nonZero_rows = np.any(masked_image_rotated, axis=1)\n",
    "                rot_nonZero_cols = np.any(masked_image_rotated, axis=0)\n",
    "                shift_nonZero_rows = np.any(masked_image_shifted, axis=1)\n",
    "                shift_nonZero_cols = np.any(masked_image_shifted, axis=0)\n",
    "\n",
    "                cropped_image = masked_image[np.ix_(non_zero_rows, non_zero_cols)]\n",
    "                cropped_rotated = masked_image_rotated[np.ix_(rot_nonZero_rows, rot_nonZero_cols)]\n",
    "                cropped_shifted = masked_image_shifted[np.ix_(shift_nonZero_rows, shift_nonZero_cols)]\n",
    "\n",
    "                processed_images.append(cropped_shifted)\n",
    "                processed_knpixvals.append(ISPi)\n",
    "                #processed_images.extend(tuple((cropped_shifted, ISPi)))\n",
    "                #print(f\"processed: {len(processed_images)}\")\n",
    "                # print(f\"Original annuli min: {cropped_image.min()}, max: {cropped_image.max()}\")\n",
    "                # print(f\"Rotated annuli min: {cropped_rotated.min()}, max: {cropped_rotated.max()}\")\n",
    "                # print(f\"Shifted annuli min: {cropped_shifted.min()}, max: {cropped_shifted.max()}\")\n",
    "\n",
    "                if plot_images:\n",
    "                    fig, axs = plt.subplots(3, 2, figsize=(10, 10), layout='tight')\n",
    "\n",
    "                    plot_image(ax=axs[0], title=f'Original', xsp_i=XSP_i, ysp_i=YSP_i, SR_m=SR_m,\n",
    "                            image_full=image_copy, image_cropped=cropped_image,\n",
    "                            masking=masked_image, annotations=None, vmin=vmin, vmax=vmax)\n",
    "                    plot_image(ax=axs[1], title=f'rotated ', xsp_i=XSP_i, ysp_i=YSP_i, SR_m=SR_m,\n",
    "                            image_full=rotated_image, masking=masked_image_rotated,\n",
    "                            image_cropped=cropped_rotated, annotations=None, vmin=vmin, vmax=vmax)\n",
    "                    plot_image(ax=axs[2], title=f\"shifted\", image_full=shifted_image,\n",
    "                            image_cropped=cropped_shifted, xsp_i=XSP_i, ysp_i=YSP_i,\n",
    "                            SR_m=SR_m, masking=masked_image_shifted, annotations=None, vmin=vmin, vmax=vmax)\n",
    "                    \n",
    "                    if annuli_save:\n",
    "                        plt.savefig(f\"../cutouts/step{step}_cutout{count}\")\n",
    "                    plt.show()\n",
    "                    count+=1\n",
    "    return processed_images, processed_knpixvals\n",
    "\n",
    "def get_data(dir):\n",
    "    '''will expand into whole dir like glob'''\n",
    "    with fits.open(dir) as hdul:\n",
    "        image_data = np.asanyarray(hdul[0].data, dtype=float)\n",
    "    return image_data\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"Normalize data\"\"\"\n",
    "    eps = 1e-8 #avoid division by zero\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "    return (data-data_min)/(data_max - data_min + eps)\n",
    "\n",
    "def turn_to_tensor(data):\n",
    "    \"\"\"Turn into tensors and unsqueese to add dimension (1 for grayscale)\n",
    "    \"\"\"\n",
    "    return torch.tensor(data, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "def create_data_loader(image_batch, knpixval ,batch_size=20):\n",
    "    #supposed to be inputs and targs\n",
    "    if not isinstance(knpixval, torch.Tensor):\n",
    "        knpixval = torch.tensor(knpixval, dtype=torch.float32)\n",
    "    \n",
    "    #stack images into a single tensor\n",
    "    print(f\"image batch type: {type(image_batch)}\")\n",
    "    print(f\"image_batch len: {len(image_batch)}\")\n",
    "    print(f\"0 ex: {image_batch[0]}\")\n",
    "    image_tensor_batch = torch.stack(image_batch)\n",
    "    print(f\"image_tensor_batch: {image_tensor_batch}\")\n",
    "    dataset = TensorDataset(image_tensor_batch, knpixval)\n",
    "    return DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "def increment_training(model, optimizer, loss_func, dataloader, epochs=5):\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=2,\n",
    "                                                gamma=0.1)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for input, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input)\n",
    "            #print(f\"output:{output.shape()}, target {targ.shape()}\")\n",
    "            target = target.view(-1,1)\n",
    "            loss = loss_func(output, target)\n",
    "            loss = loss.float()\n",
    "            print(f\"loss type: {loss.dtype}\")\n",
    "            print(f'loss: {loss}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #track loss\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "    torch.save(model.state_dict(), 'model_checkpoint.pth')\n",
    "\n",
    "MEMORY_BUFFER = []\n",
    "BUFFER_SIZE = 1000 #max buffer size\n",
    "def memory_buff_updater(new_data):\n",
    "    '''to avoid catastrophic forgetting for continuos learning.\n",
    "    look into better versions such as reservoir sampling buffer\n",
    "    or prioritized replay buffer for harder to analyze data (bright spots?)'''\n",
    "    MEMORY_BUFFER.extend(new_data)\n",
    "    if len(MEMORY_BUFFER) > BUFFER_SIZE:\n",
    "        MEMORY_BUFFER = MEMORY_BUFFER[-BUFFER_SIZE:]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensored_batch len: 362\n",
      "image batch type: <class 'list'>\n",
      "image_batch len: 362\n",
      "0 ex: tensor([[[0.0245, 0.0245, 0.2842, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.0245, 0.0245, 0.1757, 0.0245, 0.0245],\n",
      "         [0.0245, 0.0245, 0.3543, 0.3397, 0.2782, 0.2211, 0.2974, 0.1882,\n",
      "          0.1680, 0.1745, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.0245, 0.0245, 0.0245, 0.3510, 0.3125, 0.2421, 0.1633,\n",
      "          0.1826, 0.2187, 0.2245, 0.2245, 0.0245, 0.0245],\n",
      "         [0.0245, 0.0245, 0.2588, 0.2880, 0.2989, 0.2753, 0.2492, 0.2108,\n",
      "          0.2184, 0.2036, 0.2676, 0.3566, 0.3447, 0.3295, 0.2687, 0.2250,\n",
      "          0.2289, 0.2206, 0.1833, 0.1317, 0.1385, 0.2538, 0.2905, 0.2389,\n",
      "          0.3296, 0.2800, 0.2551, 0.3531, 0.2897, 0.2949, 0.2726, 0.1802,\n",
      "          0.2600, 0.1921, 0.2285, 0.2271, 0.0245, 0.0245],\n",
      "         [0.0245, 0.2582, 0.2295, 0.2537, 0.2747, 0.2677, 0.2187, 0.2203,\n",
      "          0.2458, 0.2442, 0.2713, 0.2901, 0.3445, 0.3386, 0.3459, 0.3196,\n",
      "          0.2962, 0.1811, 0.1839, 0.1317, 0.1605, 0.1755, 0.1843, 0.2095,\n",
      "          0.2094, 0.2672, 0.3029, 0.2857, 0.2800, 0.2625, 0.2271, 0.2071,\n",
      "          0.2296, 0.2098, 0.1586, 0.2210, 0.1600, 0.0245],\n",
      "         [0.0245, 0.4188, 0.3722, 0.3587, 0.2837, 0.2410, 0.2224, 0.2144,\n",
      "          0.2186, 0.2486, 0.2226, 0.2149, 0.2953, 0.2569, 0.3212, 0.2626,\n",
      "          0.2713, 0.2015, 0.1095, 0.0580, 0.0421, 0.1843, 0.2141, 0.2027,\n",
      "          0.2117, 0.2647, 0.3126, 0.2967, 0.2704, 0.2183, 0.2063, 0.2151,\n",
      "          0.2879, 0.2392, 0.1551, 0.2645, 0.2235, 0.0245],\n",
      "         [0.0245, 0.3190, 0.3395, 0.3341, 0.2975, 0.3280, 0.2689, 0.2670,\n",
      "          0.2310, 0.2415, 0.2310, 0.2793, 0.2397, 0.2796, 0.3234, 0.2691,\n",
      "          0.0431, 0.0000, 0.0098, 0.0180, 0.0071, 0.1826, 0.2351, 0.1591,\n",
      "          0.1819, 0.2905, 0.3549, 0.3838, 0.2923, 0.3076, 0.2268, 0.1473,\n",
      "          0.1993, 0.2335, 0.2361, 0.2429, 0.2215, 0.0245],\n",
      "         [0.0245, 0.4318, 0.2699, 0.3228, 0.2985, 0.2095, 0.2645, 0.3388,\n",
      "          0.2342, 0.1905, 0.2260, 0.2180, 0.2451, 0.2908, 0.3689, 0.2955,\n",
      "          0.0848, 0.0189, 0.0261, 0.0254, 0.0176, 0.0718, 0.1549, 0.1591,\n",
      "          0.1414, 0.2443, 0.3318, 0.2993, 0.2907, 0.3134, 0.2132, 0.2031,\n",
      "          0.3167, 0.3556, 0.2400, 0.2472, 0.2315, 0.0245],\n",
      "         [0.0245, 0.3295, 0.3428, 0.3728, 0.2979, 0.2898, 0.3036, 0.2784,\n",
      "          0.2245, 0.2082, 0.1816, 0.1607, 0.1991, 0.2225, 0.3149, 0.2958,\n",
      "          0.1081, 0.0124, 0.0239, 0.0231, 0.0161, 0.0764, 0.2206, 0.1881,\n",
      "          0.1758, 0.2258, 0.2884, 0.3055, 0.3072, 0.2587, 0.2243, 0.1684,\n",
      "          0.1925, 0.1896, 0.1849, 0.2560, 0.2138, 0.0245],\n",
      "         [0.3541, 0.4575, 0.4435, 0.3462, 0.3283, 0.3130, 0.2954, 0.2868,\n",
      "          0.2798, 0.2213, 0.1751, 0.2278, 0.2946, 0.2055, 0.2317, 0.2901,\n",
      "          0.1829, 0.0079, 0.0206, 0.0184, 0.0151, 0.0527, 0.2433, 0.1647,\n",
      "          0.1980, 0.2751, 0.2678, 0.1864, 0.2835, 0.2480, 0.2812, 0.2180,\n",
      "          0.2238, 0.2385, 0.2481, 0.2471, 0.2291, 0.3136],\n",
      "         [0.5201, 0.5724, 0.5603, 0.5110, 0.5237, 0.4011, 0.3384, 0.2680,\n",
      "          0.2466, 0.2418, 0.2385, 0.2295, 0.2132, 0.2238, 0.2317, 0.1914,\n",
      "          0.1507, 0.0405, 0.0620, 0.1003, 0.1234, 0.2104, 0.2432, 0.1618,\n",
      "          0.1728, 0.2309, 0.1928, 0.1483, 0.1867, 0.2074, 0.2005, 0.2007,\n",
      "          0.1801, 0.1922, 0.2247, 0.2374, 0.2701, 0.2620],\n",
      "         [0.0245, 0.0245, 1.0000, 0.8848, 0.7242, 0.4262, 0.2743, 0.2194,\n",
      "          0.1480, 0.2427, 0.2618, 0.1931, 0.1668, 0.2307, 0.2494, 0.2089,\n",
      "          0.1701, 0.1937, 0.1735, 0.1807, 0.1839, 0.1747, 0.1819, 0.2364,\n",
      "          0.2130, 0.2029, 0.2067, 0.1750, 0.1961, 0.2358, 0.2063, 0.1943,\n",
      "          0.2265, 0.1872, 0.2334, 0.2316, 0.0245, 0.0245],\n",
      "         [0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.2909, 0.2129, 0.2610, 0.2346, 0.2338, 0.2286, 0.2505,\n",
      "          0.2228, 0.1893, 0.1831, 0.1722, 0.2162, 0.2249, 0.2117, 0.1614,\n",
      "          0.1766, 0.1791, 0.1689, 0.1961, 0.2130, 0.0245, 0.0245, 0.0245,\n",
      "          0.0245, 0.0245, 0.0245, 0.0245, 0.0245, 0.0245]]])\n",
      "image_tensor_batch: tensor([[[[0.0245, 0.0245, 0.2842,  ..., 0.1757, 0.0245, 0.0245],\n",
      "          [0.0245, 0.0245, 0.3543,  ..., 0.2245, 0.0245, 0.0245],\n",
      "          [0.0245, 0.0245, 0.2588,  ..., 0.2271, 0.0245, 0.0245],\n",
      "          ...,\n",
      "          [0.5201, 0.5724, 0.5603,  ..., 0.2374, 0.2701, 0.2620],\n",
      "          [0.0245, 0.0245, 1.0000,  ..., 0.2316, 0.0245, 0.0245],\n",
      "          [0.0245, 0.0245, 0.0245,  ..., 0.0245, 0.0245, 0.0245]]],\n",
      "\n",
      "\n",
      "        [[[0.0615, 0.0615, 0.3623,  ..., 0.3592, 0.0615, 0.0615],\n",
      "          [0.0615, 0.0615, 0.3717,  ..., 0.4176, 0.0615, 0.0615],\n",
      "          [0.0615, 0.0615, 0.3561,  ..., 0.4124, 0.0615, 0.0615],\n",
      "          ...,\n",
      "          [0.4477, 0.3830, 0.4500,  ..., 0.3167, 0.2146, 0.2169],\n",
      "          [0.0615, 0.0615, 0.3505,  ..., 0.3418, 0.0615, 0.0615],\n",
      "          [0.0615, 0.0615, 0.0615,  ..., 0.0615, 0.0615, 0.0615]]],\n",
      "\n",
      "\n",
      "        [[[0.0123, 0.0123, 0.2805,  ..., 0.1835, 0.0123, 0.0123],\n",
      "          [0.0123, 0.0123, 0.1860,  ..., 0.1784, 0.0123, 0.0123],\n",
      "          [0.0123, 0.0123, 0.1757,  ..., 0.1734, 0.0123, 0.0123],\n",
      "          ...,\n",
      "          [0.5359, 0.7323, 0.7869,  ..., 0.1658, 0.1700, 0.1159],\n",
      "          [0.0123, 0.0123, 1.0000,  ..., 0.1793, 0.0123, 0.0123],\n",
      "          [0.0123, 0.0123, 0.0123,  ..., 0.0123, 0.0123, 0.0123]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0588, 0.0588, 0.3971,  ..., 0.3596, 0.0588, 0.0588],\n",
      "          [0.0588, 0.0588, 0.2864,  ..., 0.4792, 0.0588, 0.0588],\n",
      "          [0.0588, 0.0588, 0.1922,  ..., 0.4741, 0.0588, 0.0588],\n",
      "          ...,\n",
      "          [0.3488, 0.4455, 0.5695,  ..., 0.4141, 0.3331, 0.5351],\n",
      "          [0.0588, 0.0588, 0.5746,  ..., 0.5700, 0.0588, 0.0588],\n",
      "          [0.0588, 0.0588, 0.0588,  ..., 0.0588, 0.0588, 0.0588]]],\n",
      "\n",
      "\n",
      "        [[[0.0597, 0.0597, 0.4756,  ..., 0.4470, 0.0597, 0.0597],\n",
      "          [0.0597, 0.0597, 0.4442,  ..., 0.3770, 0.0597, 0.0597],\n",
      "          [0.0597, 0.0597, 0.2525,  ..., 0.3687, 0.0597, 0.0597],\n",
      "          ...,\n",
      "          [0.5038, 0.5891, 0.3984,  ..., 0.3035, 0.5832, 0.6861],\n",
      "          [0.0597, 0.0597, 0.8006,  ..., 0.4028, 0.0597, 0.0597],\n",
      "          [0.0597, 0.0597, 0.0597,  ..., 0.0597, 0.0597, 0.0597]]],\n",
      "\n",
      "\n",
      "        [[[0.0547, 0.0547, 0.4386,  ..., 0.4246, 0.0547, 0.0547],\n",
      "          [0.0547, 0.0547, 0.3667,  ..., 0.5062, 0.0547, 0.0547],\n",
      "          [0.0547, 0.0547, 0.4041,  ..., 0.2936, 0.0547, 0.0547],\n",
      "          ...,\n",
      "          [0.5914, 0.4740, 0.5782,  ..., 0.6003, 0.6779, 0.7097],\n",
      "          [0.0547, 0.0547, 0.7318,  ..., 0.2603, 0.0547, 0.0547],\n",
      "          [0.0547, 0.0547, 0.0547,  ..., 0.0547, 0.0547, 0.0547]]]])\n",
      "x torch.Size([20, 64, 3, 9])\n",
      "loss type: torch.float32\n",
      "loss: 86270.0625\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(MODEL\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m     45\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 46\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mincrement_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 199\u001b[0m, in \u001b[0;36mincrement_training\u001b[1;34m(model, optimizer, loss_func, dataloader, epochs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 199\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m#track loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\snedd\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\snedd\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "\n",
    "## this will change to run through dir like glob\n",
    "# fits_file = \"C:\\\\Users\\\\snedd\\\\work\\\\project\\\\raw_images\\\\cen_camsci1_20230316024607829066148.fits\"\n",
    "fits_file = \"cen_camsci1_20230316015110794263351.fits\"\n",
    "image_data = get_data(fits_file)\n",
    "\n",
    "srpix_inner = 87\n",
    "srpix_outer = 97\n",
    "orDPixAng = 22.5\n",
    "wedge_angle = np.radians(orDPixAng)\n",
    "planet_radius = 2.5\n",
    "steps = int(2 * np.pi / wedge_angle)\n",
    "\n",
    "global_min = 0\n",
    "global_max = 1000\n",
    "# print(f\"global min, max: {global_min}, {global_max}\")\n",
    "\n",
    "\n",
    "mask_region = np.zeros_like(image_data)\n",
    "cy, cx = (float(image_data.shape[1] - 1) / 2, float(image_data.shape[0] - 1) / 2)\n",
    "y, x = np.ogrid[:image_data.shape[0], :image_data.shape[1]]\n",
    "mask = ((x - cx) ** 2 + (y - cy) ** 2) <= (srpix_outer ** 2)\n",
    "mask_region[mask] = 1\n",
    "\n",
    "processed_images = []\n",
    "count = 0\n",
    "for step in range(steps):\n",
    "    rotation_angle = np.degrees(step * wedge_angle)\n",
    "    rotated_img = rotate(image_data, angle=rotation_angle, center=(cy, cx), order=3, preserve_range=True)\n",
    "    #processed_batch will be a list of tuples(img_data, knpixval)\n",
    "    processed_batch, knpixvals = process_search_region(image=rotated_img, srpix_inner=srpix_inner, srpix_outer=srpix_outer,\n",
    "                                            wedge_angle=wedge_angle, vmin=global_min, vmax=global_max,\n",
    "                                            step=step, plot_images=False)\n",
    "    \n",
    "    #normalize_data\n",
    "    processed_batch = [normalize_data(data) for data in processed_batch]\n",
    "    #turn to tensors\n",
    "    tensored_batch = [turn_to_tensor(data) for data in processed_batch]\n",
    "    print(f\"tensored_batch len: {len(tensored_batch)}\")\n",
    "    #get dataloader\n",
    "    dataloader = create_data_loader(tensored_batch, torch.tensor(knpixvals))\n",
    "    # send the batch to model\n",
    "    #model params\n",
    "    MODEL = CNN()\n",
    "    optimizer = torch.optim.Adam(MODEL.parameters(), lr=1e-4)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    result = increment_training(model=MODEL, optimizer=optimizer,\n",
    "                       loss_func=loss_func, dataloader=dataloader)\n",
    "    \n",
    "    print(result)\n",
    "\n",
    "    print(f'step {step}.')\n",
    "    if count >= 2:\n",
    "        break\n",
    "    count += 1\n",
    "# can break off to either save or forward to training\n",
    "    # plt.imshow(resized_cutout_planet, cmap='gray')\n",
    "    # plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
